{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import math\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.utils.tensorboard as tensorboard\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from dataset.config import *\n",
    "from dataset import *\n",
    "from dataset.config import voc as cfg\n",
    "from dataset.voc0712 import VOCDetection\n",
    "from ssd.ssd import build_ssd\n",
    "from ssd.multiloss import MultiLoss\n",
    "from transform.augmentation import SSDAugmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    losses, losses_loc, losses_conf = 0, 0, 0\n",
    "\n",
    "    for i, (imgs, targets) in enumerate(train_loader):\n",
    "        img = imgs.to(device)\n",
    "        targets = [{k: y.to(device) for k, y in t.items() } for t in targets]\n",
    "\n",
    "        outputs = model(img)\n",
    "        loss_loc, loss_conf = criterion(outputs, targets)\n",
    "        loss = loss_loc + args.alpha * loss_conf\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss, acc\n",
    "        losses_loc += loss_loc.item()\n",
    "        losses_conf += loss_conf.item()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len_train, losses_loc / len_train, losses_conf / len_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid():\n",
    "    losses, losses_loc, losses_conf = 0, 0, 0\n",
    "\n",
    "    for i, (imgs, targets) in enumerate(test_loader):\n",
    "        img = imgs.to(device)\n",
    "        targets = [{k: y.to(device) for k, y in t.items() } for t in targets]\n",
    "\n",
    "        outputs = model(img)\n",
    "        loss_loc, loss_conf = criterion(outputs, targets)\n",
    "        loss = loss_loc + args.alpha * loss_conf\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss, acc\n",
    "        losses_loc += loss_loc.item()\n",
    "        losses_conf += loss_conf.item()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len_test, losses_loc / len_test, losses_conf / len_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.xavier_uniform_(m.weight.data)\n",
    "            m.bias.data.zero_()\n",
    "\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, 0, .01)\n",
    "            nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_decay(epoch):\n",
    "    if epoch < 100:\n",
    "        return args.lr\n",
    "    elif epoch < 150:\n",
    "        return args.lr * .1\n",
    "    else:\n",
    "        return args.lr * .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[0.2200, 0.1365, 0.8271, 0.9948]]), 'labels': tensor([14])}, {'boxes': tensor([[0.3534, 0.4939, 0.4007, 0.7475],\n",
      "        [0.3981, 0.4839, 0.4875, 0.7497],\n",
      "        [0.4325, 0.4861, 0.4867, 0.7653],\n",
      "        [0.3646, 0.5217, 0.4282, 0.7620],\n",
      "        [0.2941, 0.4939, 0.3594, 0.7631],\n",
      "        [0.3052, 0.5239, 0.3663, 0.7709]]), 'labels': tensor([14, 14, 14, 14, 14, 14])}, {'boxes': tensor([[0.6203, 0.3220, 1.0000, 0.8452],\n",
      "        [0.6842, 0.0588, 1.0000, 0.3406]]), 'labels': tensor([14, 15])}, {'boxes': tensor([[0.0000, 0.6970, 0.3854, 1.0000],\n",
      "        [0.9101, 0.9129, 0.9465, 1.0000]]), 'labels': tensor([ 5, 14])}]\n",
      "torch.Size([4, 512, 40, 40])\n",
      "torch.Size([4, 1024, 20, 20])\n",
      "torch.Size([4, 512, 10, 10])\n",
      "torch.Size([4, 256, 5, 5])\n",
      "torch.Size([4, 256, 3, 3])\n",
      "torch.Size([4, 256, 1, 1])\n",
      "torch.Size([4, 6400, 21]) torch.Size([4, 6400, 4])\n",
      "torch.Size([4, 2400, 21]) torch.Size([4, 2400, 4])\n",
      "torch.Size([4, 600, 21]) torch.Size([4, 600, 4])\n",
      "torch.Size([4, 150, 21]) torch.Size([4, 150, 4])\n",
      "torch.Size([4, 36, 21]) torch.Size([4, 36, 4])\n",
      "torch.Size([4, 4, 21]) torch.Size([4, 4, 4])\n",
      "[{'boxes': tensor([[0.0098, 0.5564, 0.3301, 0.9948],\n",
      "        [0.3536, 0.6640, 0.7171, 0.9948],\n",
      "        [0.6896, 0.7165, 0.9411, 0.9948]]), 'labels': tensor([14, 14, 14])}, {'boxes': tensor([[0.2743, 0.4624, 0.5512, 0.7632]]), 'labels': tensor([6])}, {'boxes': tensor([[0.8064, 0.5347, 1.0000, 0.6763]]), 'labels': tensor([3])}, {'boxes': tensor([[0.2718, 0.3019, 0.8568, 1.0000]]), 'labels': tensor([7])}]\n",
      "torch.Size([4, 512, 40, 40])\n",
      "torch.Size([4, 1024, 20, 20])\n",
      "torch.Size([4, 512, 10, 10])\n",
      "torch.Size([4, 256, 5, 5])\n",
      "torch.Size([4, 256, 3, 3])\n",
      "torch.Size([4, 256, 1, 1])\n",
      "torch.Size([4, 6400, 21]) torch.Size([4, 6400, 4])\n",
      "torch.Size([4, 2400, 21]) torch.Size([4, 2400, 4])\n",
      "torch.Size([4, 600, 21]) torch.Size([4, 600, 4])\n",
      "torch.Size([4, 150, 21]) torch.Size([4, 150, 4])\n",
      "torch.Size([4, 36, 21]) torch.Size([4, 36, 4])\n",
      "torch.Size([4, 4, 21]) torch.Size([4, 4, 4])\n",
      "[{'boxes': tensor([[0.7454, 0.3942, 0.9239, 0.5165]]), 'labels': tensor([3])}, {'boxes': tensor([[0.0020, 0.1547, 0.1780, 0.9413],\n",
      "        [0.2300, 0.2240, 0.5440, 0.4987]]), 'labels': tensor([14,  6])}, {'boxes': tensor([[0.2115, 0.0000, 1.0000, 0.3200]]), 'labels': tensor([7])}, {'boxes': tensor([[0.4527, 0.4079, 0.7593, 0.8377]]), 'labels': tensor([6])}]\n",
      "torch.Size([4, 512, 40, 40])\n",
      "torch.Size([4, 1024, 20, 20])\n",
      "torch.Size([4, 512, 10, 10])\n",
      "torch.Size([4, 256, 5, 5])\n",
      "torch.Size([4, 256, 3, 3])\n",
      "torch.Size([4, 256, 1, 1])\n",
      "torch.Size([4, 6400, 21]) torch.Size([4, 6400, 4])\n",
      "torch.Size([4, 2400, 21]) torch.Size([4, 2400, 4])\n",
      "torch.Size([4, 600, 21]) torch.Size([4, 600, 4])\n",
      "torch.Size([4, 150, 21]) torch.Size([4, 150, 4])\n",
      "torch.Size([4, 36, 21]) torch.Size([4, 36, 4])\n",
      "torch.Size([4, 4, 21]) torch.Size([4, 4, 4])\n",
      "[{'boxes': tensor([[0.0000, 0.1492, 0.6243, 0.6825],\n",
      "        [0.7966, 0.4095, 1.0000, 1.0000]]), 'labels': tensor([18,  8])}, {'boxes': tensor([[0.7483, 0.5750, 0.9021, 0.6812]]), 'labels': tensor([6])}, {'boxes': tensor([[0.0000, 0.2843, 0.5995, 1.0000]]), 'labels': tensor([7])}, {'boxes': tensor([[0.1593, 0.2112, 0.8744, 0.8799]]), 'labels': tensor([7])}]\n",
      "torch.Size([4, 512, 40, 40])\n",
      "torch.Size([4, 1024, 20, 20])\n",
      "torch.Size([4, 512, 10, 10])\n",
      "torch.Size([4, 256, 5, 5])\n",
      "torch.Size([4, 256, 3, 3])\n",
      "torch.Size([4, 256, 1, 1])\n",
      "torch.Size([4, 6400, 21]) torch.Size([4, 6400, 4])\n",
      "torch.Size([4, 2400, 21]) torch.Size([4, 2400, 4])\n",
      "torch.Size([4, 600, 21]) torch.Size([4, 600, 4])\n",
      "torch.Size([4, 150, 21]) torch.Size([4, 150, 4])\n",
      "torch.Size([4, 36, 21]) torch.Size([4, 36, 4])\n",
      "torch.Size([4, 4, 21]) torch.Size([4, 4, 4])\n",
      "[{'boxes': tensor([[0.0020, 0.2400, 0.9320, 0.6800]]), 'labels': tensor([3])}, {'boxes': tensor([[0.1548, 0.2201, 0.7336, 0.9099]]), 'labels': tensor([11])}, {'boxes': tensor([[0.1360, 0.3067, 0.6640, 0.9227]]), 'labels': tensor([12])}, {'boxes': tensor([[0.3060, 0.6712, 0.3940, 0.9457],\n",
      "        [0.3920, 0.6739, 0.4680, 0.9484],\n",
      "        [0.4700, 0.6739, 0.5460, 0.9457],\n",
      "        [0.5600, 0.6739, 0.6340, 0.9511],\n",
      "        [0.6400, 0.6712, 0.7160, 0.9538],\n",
      "        [0.7180, 0.6712, 0.8120, 0.9538],\n",
      "        [0.8060, 0.6712, 0.8960, 0.9592],\n",
      "        [0.8900, 0.6658, 0.9900, 0.9538]]), 'labels': tensor([8, 8, 8, 8, 8, 8, 8, 8])}]\n",
      "torch.Size([4, 512, 40, 40])\n",
      "torch.Size([4, 1024, 20, 20])\n",
      "torch.Size([4, 512, 10, 10])\n",
      "torch.Size([4, 256, 5, 5])\n",
      "torch.Size([4, 256, 3, 3])\n",
      "torch.Size([4, 256, 1, 1])\n",
      "torch.Size([4, 6400, 21]) torch.Size([4, 6400, 4])\n",
      "torch.Size([4, 2400, 21]) torch.Size([4, 2400, 4])\n",
      "torch.Size([4, 600, 21]) torch.Size([4, 600, 4])\n",
      "torch.Size([4, 150, 21]) torch.Size([4, 150, 4])\n",
      "torch.Size([4, 36, 21]) torch.Size([4, 36, 4])\n",
      "torch.Size([4, 4, 21]) torch.Size([4, 4, 4])\n",
      "[{'boxes': tensor([[0.4620, 0.0880, 0.8700, 0.9013]]), 'labels': tensor([12])}, {'boxes': tensor([[0.1911, 0.5866, 0.4045, 0.8399],\n",
      "        [0.4324, 0.4004, 0.6248, 0.8603]]), 'labels': tensor([19, 14])}, {'boxes': tensor([[0.0000, 0.0000, 0.9680, 0.9964]]), 'labels': tensor([14])}, {'boxes': tensor([[0.1420, 0.3387, 0.9980, 0.9973],\n",
      "        [0.6860, 0.0000, 0.9300, 0.4160],\n",
      "        [0.3820, 0.0000, 0.6020, 0.2853],\n",
      "        [0.1420, 0.2053, 0.2720, 0.4000]]), 'labels': tensor([17,  8, 15, 19])}]\n",
      "torch.Size([4, 512, 40, 40])\n",
      "torch.Size([4, 1024, 20, 20])\n",
      "torch.Size([4, 512, 10, 10])\n",
      "torch.Size([4, 256, 5, 5])\n",
      "torch.Size([4, 256, 3, 3])\n",
      "torch.Size([4, 256, 1, 1])\n",
      "torch.Size([4, 6400, 21]) torch.Size([4, 6400, 4])\n",
      "torch.Size([4, 2400, 21]) torch.Size([4, 2400, 4])\n",
      "torch.Size([4, 600, 21]) torch.Size([4, 600, 4])\n",
      "torch.Size([4, 150, 21]) torch.Size([4, 150, 4])\n",
      "torch.Size([4, 36, 21]) torch.Size([4, 36, 4])\n",
      "torch.Size([4, 4, 21]) torch.Size([4, 4, 4])\n",
      "[{'boxes': tensor([[0.3306, 0.0946, 0.4798, 0.2297]]), 'labels': tensor([13])}, {'boxes': tensor([[0.0397, 0.5196, 0.3639, 0.9924],\n",
      "        [0.3231, 0.5544, 0.5986, 0.9924],\n",
      "        [0.5340, 0.4622, 0.6054, 0.6752]]), 'labels': tensor([14, 14, 14])}, {'boxes': tensor([[0.0269, 0.0000, 0.9103, 0.8728]]), 'labels': tensor([2])}, {'boxes': tensor([[0.4986, 0.7382, 0.5484, 1.0000],\n",
      "        [0.6056, 0.7445, 0.6573, 1.0000],\n",
      "        [0.6657, 0.7445, 0.7221, 1.0000],\n",
      "        [0.7277, 0.7320, 0.7887, 1.0000],\n",
      "        [0.7033, 0.6928, 0.7493, 1.0000],\n",
      "        [0.5465, 0.7273, 0.6075, 0.8918],\n",
      "        [0.5850, 0.7288, 0.6188, 0.8213],\n",
      "        [0.7850, 0.7492, 0.8113, 0.7994]]), 'labels': tensor([ 4,  4,  4,  4,  4, 14, 14, 14])}]\n",
      "torch.Size([4, 512, 40, 40])\n",
      "torch.Size([4, 1024, 20, 20])\n",
      "torch.Size([4, 512, 10, 10])\n",
      "torch.Size([4, 256, 5, 5])\n",
      "torch.Size([4, 256, 3, 3])\n",
      "torch.Size([4, 256, 1, 1])\n",
      "torch.Size([4, 6400, 21]) torch.Size([4, 6400, 4])\n",
      "torch.Size([4, 2400, 21]) torch.Size([4, 2400, 4])\n",
      "torch.Size([4, 600, 21]) torch.Size([4, 600, 4])\n",
      "torch.Size([4, 150, 21]) torch.Size([4, 150, 4])\n",
      "torch.Size([4, 36, 21]) torch.Size([4, 36, 4])\n",
      "torch.Size([4, 4, 21]) torch.Size([4, 4, 4])\n",
      "[{'boxes': tensor([[0.0000, 0.0000, 1.0000, 0.8675]]), 'labels': tensor([18])}, {'boxes': tensor([[0.1088, 0.6463, 0.2568, 0.7460]]), 'labels': tensor([14])}, {'boxes': tensor([[0.0000, 0.3278, 0.5158, 0.9484]]), 'labels': tensor([14])}, {'boxes': tensor([[0.1231, 0.1947, 0.5000, 0.8726],\n",
      "        [0.4815, 0.2159, 0.8431, 0.8726]]), 'labels': tensor([14, 14])}]\n",
      "torch.Size([4, 512, 40, 40])\n",
      "torch.Size([4, 1024, 20, 20])\n",
      "torch.Size([4, 512, 10, 10])\n",
      "torch.Size([4, 256, 5, 5])\n",
      "torch.Size([4, 256, 3, 3])\n",
      "torch.Size([4, 256, 1, 1])\n",
      "torch.Size([4, 6400, 21]) torch.Size([4, 6400, 4])\n",
      "torch.Size([4, 2400, 21]) torch.Size([4, 2400, 4])\n",
      "torch.Size([4, 600, 21]) torch.Size([4, 600, 4])\n",
      "torch.Size([4, 150, 21]) torch.Size([4, 150, 4])\n",
      "torch.Size([4, 36, 21]) torch.Size([4, 36, 4])\n",
      "torch.Size([4, 4, 21]) torch.Size([4, 4, 4])\n",
      "[{'boxes': tensor([[0.0374, 0.2315, 1.0000, 1.0000],\n",
      "        [0.8037, 0.0161, 1.0000, 0.6592]]), 'labels': tensor([12, 14])}, {'boxes': tensor([[0.5697, 0.3864, 1.0000, 0.9661],\n",
      "        [0.0000, 0.3186, 0.4364, 0.9051],\n",
      "        [0.0000, 0.2678, 0.3152, 0.6780],\n",
      "        [0.0000, 0.2712, 1.0000, 0.9525]]), 'labels': tensor([ 8,  8,  8, 10])}, {'boxes': tensor([[0.0000, 0.2074, 0.3599, 1.0000]]), 'labels': tensor([14])}, {'boxes': tensor([[0.4853, 0.6002, 0.8033, 1.0000]]), 'labels': tensor([7])}]\n",
      "torch.Size([4, 512, 40, 40])\n",
      "torch.Size([4, 1024, 20, 20])\n",
      "torch.Size([4, 512, 10, 10])\n",
      "torch.Size([4, 256, 5, 5])\n",
      "torch.Size([4, 256, 3, 3])\n",
      "torch.Size([4, 256, 1, 1])\n",
      "torch.Size([4, 6400, 21]) torch.Size([4, 6400, 4])\n",
      "torch.Size([4, 2400, 21]) torch.Size([4, 2400, 4])\n",
      "torch.Size([4, 600, 21]) torch.Size([4, 600, 4])\n",
      "torch.Size([4, 150, 21]) torch.Size([4, 150, 4])\n",
      "torch.Size([4, 36, 21]) torch.Size([4, 36, 4])\n",
      "torch.Size([4, 4, 21]) torch.Size([4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-root', type=str, default='/Users/miyasatotakaya/Datasets/pascalvoc/VOCdevkit/')\n",
    "    parser.add_argument('-gpu', type=bool, default=False, help='use gpu or not')\n",
    "    parser.add_argument('-lr', type=float, default=.01, help='initial learning rate')\n",
    "    parser.add_argument('-alpha', type=float, default=1., help='initial learning rate')\n",
    "    parser.add_argument('-batch', type=int, default=32, help='batch size for dataloader')\n",
    "    parser.add_argument('-size', type=int, default=32, help='image size for datasets')\n",
    "    parser.add_argument('-ch', type=int, default=3, help='input channels')\n",
    "    parser.add_argument('-class_num', type=int, default=10, help='data class')\n",
    "    parser.add_argument('-epoch', type=int, default=200, help='training epoch')\n",
    "    parser.add_argument('-resume', type=str, help='load weight')\n",
    "    parser.add_argument('-save', type=str, default='save/ckpt.pth', help='saved models')\n",
    "    parser.add_argument('-fmodel', type=str, default='save/fmodel', help='final saved models')\n",
    "    parser.add_argument('-lr_scheduler')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    os.makedirs('save', exist_ok=True)\n",
    "    \n",
    "    ## Loading_data\n",
    "    train_data = VOCDetection(args.root, transform=SSDAugmentation(cfg['min_dim'], MEANS), image_sets=[('2012', 'train')])\n",
    "    test_data = VOCDetection(args.root, transform=SSDAugmentation(cfg['min_dim'], MEANS), image_sets=[('2012', 'val')])\n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size=4, shuffle=True, num_workers=2, collate_fn=detection_collate)\n",
    "    test_loader = DataLoader(test_data, batch_size=4, shuffle=False, num_workers=2, collate_fn=detection_collate)\n",
    "    \n",
    "    len_train = len(train_data)\n",
    "    len_test = len(test_data)\n",
    "    \n",
    "    ## Build model\n",
    "    model = build_ssd('train', cfg['min_dim'], cfg['class_num'])\n",
    "    \n",
    "    ## load weights or init weights\n",
    "    if args.resume:\n",
    "        checkpoints = torch.load(args.resume)\n",
    "        weights = checkpoints['weights']\n",
    "        start = checkpoints['epoch']\n",
    "        max_loss = checkpoints['loss']\n",
    "        model.load_state_dict(weights)\n",
    "        \n",
    "    else:\n",
    "        model.apply(init_weight)\n",
    "        \n",
    "        start = 0\n",
    "        max_loss = math.inf\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    ##  setting\n",
    "    criterion = MultiLoss(num_classes=cfg['class_num'], device=device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=.9)\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer, lr_lambda=param_decay)\n",
    "    \n",
    "    ## training model\n",
    "    for epoch in range(start, args.epoch):\n",
    "        tl, tl_loc, tl_conf = train()\n",
    "        vl, vl_loc, vl_conf = valid()\n",
    "        \n",
    "        print(epoch + 1, tl, tl_loc, tl_conf, vl, vl_loc, vl_conf)\n",
    "        \n",
    "        writer.add_scalars( 'data/loss', \n",
    "                           {'train/loss': tl, 'train/loss_loc': tl_loc, 'train/loss_conf': tl_conf,\n",
    "                            'val/loss': vl, 'val/loss_loc': vl_loc, 'val/loss_conf': vl_conf},\n",
    "                           epoch + 1 )\n",
    "        \n",
    "        torch.save({ 'weights': model.state_dict(), 'loss': vl, 'epoch': epoch}, args.save)\n",
    "        if max_loss > vl:\n",
    "            torch.save({ 'weights': model.state_dict(), 'loss': vl, 'epoch': epoch}, args.fmodel + '.pth')\n",
    "    \n",
    "    ##  suii\n",
    "    writer.export_scalars_to_json(args.fmodel + '.json')\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
